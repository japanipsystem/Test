{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/japanipsystem/Test/blob/master/AT_ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#機械学習を使った株価予想\n",
        "Copyright (c) 2024 Takanori Adachi. All rights reserved."
      ],
      "metadata": {
        "id": "HqkjvMaypDz8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**参考文献**\n",
        "\n",
        "1. Francois Chollet, \"[Deep Learning with Python, 2nd Ed.](https://www.amazon.co.jp/Learning-Python-Second-François-Chollet/dp/1617296864/ref=sr_1_1?__mk_ja_JP=カタカナ&crid=3S64D6OAIA354&keywords=deep+learning+with+python&qid=1653014306&sprefix=deep+learning+with+python%2Caps%2C165&sr=8-1)\", Manning Pub. (2021).\n",
        "(Kerasの開発者が著したこの本は，深層学習の標準的教科書です．)\n",
        "\n",
        "1.  [Keras API reference](https://keras.io/api/).\n",
        "(Kerasのアプリケーション・インターフェイス (API) を解説しています．\n",
        "実例も豊富です．)\n",
        "\n",
        "1. [Keras GitHub](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/keras).\n",
        "(上記API解説文書には載っていない細かい仕様を調べようとすると，Kerasのソースコードにあたるしかありません．\n",
        "このGitHubはそのときに参照する場所です．)\n",
        "\n",
        "1. [How to Predict Stock Prices in Python using TensorFlow 2 and Keras](https://www.thepythoncode.com/article/stock-price-prediction-in-python-using-tensorflow-2-and-keras).\n",
        "(Kerasを使って株価予測をしようという試みで，この講義資料執筆でも大いに参考にしました．)\n",
        "\n",
        "1. [Yohoo_fin Documentation](http://theautomatic.net/yahoo_fin-documentation/).\n",
        "(本講義で利用するデータ・ソースである yahoo_fin の標準文書です．)\n",
        "\n",
        "1. David M. Beazley, \"[Python Distilled](https://www.amazon.co.jp/Python-Essential-Reference-Developers-Library/dp/0134173279/ref=sr_1_1?__mk_ja_JP=カタカナ&crid=3JW4WHS94JOWC&keywords=Python+Distilled&qid=1653023273&s=english-books&sprefix=python+distille%2Cenglish-books%2C172&sr=1-1)\", Addison Wesley\n",
        "(2021).\n",
        "(足立が利用している Python の教科書のなかでは，もっとも優れていると思います．)\n",
        "\n",
        "1. David Amos, \"[Object-Oriented Programming (OOP) in Python 3](https://realpython.com/python3-object-oriented-programming/)\"\n",
        "(手っ取り早く「オブジェクト指向プログラミング」を学びたいときに便利なサイトです．)\n",
        "\n",
        "1. Kuroiku, [深層学習シリーズ](https://qiita.com/kuroitu/items/221e8c477ffdd0774b6b#深層学習シリーズ).\n",
        "(ライブラリを使わずに，深層学習を順番に考えスクラッチから作る Python コードで解説しようとしています．)\n",
        "\n",
        "1. Aurelien Geron, \"[Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow (2nd ed.)](https://www.amazon.co.jp/gp/product/1492032646/ref=ppx_yo_dt_b_asin_title_o01_s00?ie=UTF8&psc=1)\", O'Reilly 2019.\n",
        "(深層学習に限らず機械学習一般について詳しく網羅された，定評のある教科書です．)\n",
        "\n",
        "1. 大槻兼資, 秋葉拓哉, \"[アルゴリズムとデータ構造](https://www.amazon.co.jp/gp/product/4065128447/ref=ppx_yo_dt_b_asin_title_o01_s00?ie=UTF8&psc=1)\", 講談社 2020.\n",
        "(プログラミング技術を向上させたいならば，アルゴリズムとデータ構造の学習は必須と思います．この本は内容も豊富で解説も平易です．ただし，Python ではなく C++ を使っています．)"
      ],
      "metadata": {
        "id": "bOz1Ar6loj-Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. データの前処理**\n",
        "\n",
        "\n",
        "最初に，日次株価データを取り込むために，ライブラリ yahoo_fin をインストールします．"
      ],
      "metadata": {
        "id": "58iv9JNLoWJG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMXvZkYToSHD"
      },
      "outputs": [],
      "source": [
        "pip install yahoo_fin"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "たとえば Google（GOOGL)の日次データを取り込んでみると，以下のようになります．"
      ],
      "metadata": {
        "id": "4tknHuGKpvNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from yahoo_fin import stock_info as si\n",
        "df = si.get_data('GOOGL')\n",
        "df"
      ],
      "metadata": {
        "id": "QuiBEZpJpwVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "各レコードは7つのカラムから成っているのがわかるでしょう．このうち終値を示す特徴量としては，株式分割や配当分配を考慮した adjusted close を利用します．"
      ],
      "metadata": {
        "id": "SElOP4D9p_4E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**課題**\n",
        "Ticker を変更して（例えば \"AAPL\", \"META\", \"AMZN\" など），データを取り込んでみよ．"
      ],
      "metadata": {
        "id": "uhwrlEEVqNRa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "まずは，株価データを格納するクラス SPData を実装するために必要なライブラリを import します．"
      ],
      "metadata": {
        "id": "053M7vD-qYbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "MvNVEW8IqA2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ticker を指定して，Yahoo Finance から読み込んだデータは，クラス SPData のインスタンスとして管理されます．\n",
        "\n",
        "SPData は以下のとおりです．"
      ],
      "metadata": {
        "id": "PRMu4PIcqkaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# individual stock price data class\n",
        "class SPData(object):\n",
        "    def __init__(self, ticker):\n",
        "        \"\"\"\n",
        "        Loads data from Yahoo Finance source, as well as scaling.\n",
        "        Params:\n",
        "            ticker (str/pd.DataFrame): the ticker you want to load, examples include AAPL, TESL, etc.\n",
        "        \"\"\"\n",
        "\n",
        "        # save parameters\n",
        "        self.ticker = ticker\n",
        "\n",
        "        # the list of features to use to feed into the model, default is everything grabbed from yahoo_fin\n",
        "        self.feature_columns = ['adjclose', 'volume', 'open', 'high', 'low']\n",
        "\n",
        "        # load data from Yahoo Finance source\n",
        "        self.df = self.load_df()\n",
        "\n",
        "    def load_df(self): # load data from Yahoo Finance source\n",
        "        df = si.get_data(self.ticker) # load from yahoo_fin library\n",
        "\n",
        "        self.orig_df = df.copy() # the original dataframe itself\n",
        "        self.save_orig_df()\n",
        "\n",
        "        # add date as a column\n",
        "        if \"date\" not in df.columns:\n",
        "            df[\"date\"] = df.index\n",
        "        return(df)\n",
        "\n",
        "    def save_orig_df(self): # save the original dataframe\n",
        "        date_now = time.strftime(\"%Y-%m-%d\")\n",
        "        self.ticker_data_filename = os.path.join(\"data\", \"%s_%s.csv\" % (self.ticker, date_now))\n",
        "        self.orig_df.to_csv(self.ticker_data_filename)\n",
        "\n",
        "    # enhance each daily data with its history whose length is n_steps\n",
        "    def preprocess_data(self, scaler = preprocessing.StandardScaler(), lookup_step=1, n_steps=50):\n",
        "        \"\"\"\n",
        "        Params:\n",
        "            scaler : preprocessing scaler, default is preprocessing.StandardScaler()\n",
        "            # See https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing\n",
        "            lookup_step (int): the future lookup step to predict, default is 1 (e.g next day)\n",
        "            n_steps (int): the historical sequence length (i.e window size) used to predict, default is 50\n",
        "        \"\"\"\n",
        "        self.scaler = scaler\n",
        "        self.lookup_step = lookup_step\n",
        "        self.n_steps = n_steps\n",
        "\n",
        "        # scale the data (prices)\n",
        "        for column in self.feature_columns:\n",
        "            self.df[column] = self.scaler.fit_transform(np.expand_dims(self.df[column].values, axis=1))\n",
        "\n",
        "        df = self.df.copy()\n",
        "\n",
        "        # add 'future' column\n",
        "        self.add_future(df)\n",
        "\n",
        "        # convert each day data to a historical \"n_steps\"s data\n",
        "        sequence_data = []\n",
        "        sequences = deque(maxlen=self.n_steps)\n",
        "        for entry, target in zip(df[self.feature_columns + [\"date\"]].values, df['future'].values):\n",
        "            sequences.append(entry)\n",
        "            if len(sequences) == self.n_steps:\n",
        "                sequence_data.append([np.array(sequences), target])\n",
        "\n",
        "        # get the last sequence by appending the last `n_step` sequence with `lookup_step` sequence\n",
        "        # for instance, if n_steps=50 and lookup_step=10, last_sequence should be of 60 (that is 50+10) length\n",
        "        # this last_sequence will be used to predict future stock prices that are not available in the dataset\n",
        "        last_sequence_list = list([s[:len(self.feature_columns)] for s in sequences]) + list(self.last_sequence)\n",
        "        self.last_sequence = np.array(last_sequence_list).astype(np.float32)\n",
        "\n",
        "        # construct the X's and y's\n",
        "        X, y = [], []\n",
        "        for seq, target in sequence_data:\n",
        "            X.append(seq) # shape(seq)= (n_steps) x (# of feature_colums +1)\n",
        "            y.append(target)\n",
        "        # convert to numpy arrays\n",
        "        self.X = np.array(X) # array of 'seq'\n",
        "        self.y = np.array(y) # array of 'target'\n",
        "\n",
        "    def add_future(self, df):\n",
        "        # add future as a column\n",
        "        df['future'] = df['adjclose'].shift(-self.lookup_step)\n",
        "        # last `lookup_step` columns contains NaN in future column\n",
        "        # save them as \"self.last_sequence\" before droping NaNs (the value of 'future' at the last index is 'NaN')\n",
        "        self.last_sequence = np.array(df[self.feature_columns].tail(self.lookup_step)) # exclude 'date' field\n",
        "        df.dropna(inplace=True) # drop NaNs"
      ],
      "metadata": {
        "id": "G1E69-7cqlSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "前半は，yahoo_fin からのデータの読み込みです．\n",
        "\n",
        "self.future_columns には，データのカラムの中から，特徴量として利用するカラムのリストを指定しています．\n",
        "メソッド load_df は，読み込んだデータを加工せずに，save_orig_df を使ってファイルに格納してます．\n",
        "このとき，格納するフォルダ \"./data\" は main を起動するときに，（必要ならば）作成するようにします．\n",
        "\n",
        "その後，インデックスとなっている日付情報を，新しいカラム \"date\" に格納します．"
      ],
      "metadata": {
        "id": "V7XrIvFsqvKB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "前半で読み込んだデータは，「日数 $\\times$ レコード（カラムの集合）」の2次元の構造を持っています．\n",
        "これを，この各レコードに，予め決めた日数（n_steps) の過去データを含めることによって，\n",
        "「日数 $\\times$ ヒストリー $\\times$ レコード（カラムの集合）」\n",
        "の3次元データを作成するのが，SPData の後半の仕事です．"
      ],
      "metadata": {
        "id": "JCVUX5aOq2Vp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "後半は，メソッド preprocess_data で，これはデータ構築に用いられます．\n",
        "\n",
        "ここでは副作用を避けるため，まずはデータをコピーします．\n",
        "\n",
        "続いてスケーリングです．\n",
        "学習に使うデータは，各カラム毎にスケールが違うと，問題が生じます．\n",
        "そのため，\n",
        "\n",
        "1. 平均0，標準偏差1に標準化する\n",
        "1. 最大値を1，最小値を0となるように線形変換する\n",
        "\n",
        "などのスケーリングをする必要があります．\n",
        "メソッド \"scale_data\" は，sklearn.preprocessing を使って，これを行います．\n",
        "\n",
        "\n",
        "\n",
        "予測すべき日付の価格を格納したカラム \"future\" をメソッドadd_future を使って追加します．\n",
        "このとき，引数の lookup_step で，何日先を予測するかを指定します．\n",
        "なお，\"future\" カラムの価格は教師データとして使われます．\n",
        "\n",
        "次は，本命の3次元データの作成です．\n",
        "\n",
        "リスト sequence_data\n",
        "には，\n",
        "[ n_steps日分の特徴量カラムの全体, 教師データ ]\n",
        "のペアを「日数 - n_steps」分格納し，\n",
        "それを Numpy配列にした結果を\n",
        "self.X と self.y としています．"
      ],
      "metadata": {
        "id": "wqopHr9oq9X2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**課題**.\n",
        "scaler の使用方法や種類を[マニュアル](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing)で調べよ．"
      ],
      "metadata": {
        "id": "j3pCEr8TrFWi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DqnllXNZrJxp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "では，ここまでの部分を動かして，実際に，self.X と self.y がどのような形をしているか見てみましょう．\n",
        "\n",
        "まずは，乱数の種の初期化と，必要なフォルダの生成を行う補助関数を定義します．"
      ],
      "metadata": {
        "id": "v-tiFpfBrPaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Miscellaneous Functions\n",
        "\n",
        "def set_seeds(): # set seeds, so we can get the same results after rerunning several times\n",
        "    np.random.seed(314)\n",
        "\n",
        "def create_folders(): # create folders if they do not exist\n",
        "    if not os.path.isdir(\"data\"):\n",
        "        os.mkdir(\"data\")"
      ],
      "metadata": {
        "id": "lnuSahw1rLf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "では，走らせてみましょう．"
      ],
      "metadata": {
        "id": "NIIqfCk0rbdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    set_seeds()\n",
        "    create_folders()\n",
        "\n",
        "    ### load the data ###\n",
        "\n",
        "    #ticker = \"GOOGL\"\n",
        "    #ticker = \"AAPL\"\n",
        "    ticker = \"META\" # ex-FB\n",
        "    #ticker = \"AMZN\"\n",
        "    data = SPData(ticker)\n",
        "\n",
        "    # for scaler, see https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing\n",
        "    SCALER = preprocessing.MinMaxScaler()   # fit the data within [0,1]\n",
        "    #SCALER = preprocessing.StandardScaler()    # normalize the data\n",
        "    data.preprocess_data(scaler=SCALER, lookup_step=15, n_steps=50)\n",
        "\n",
        "    print('data.X=', data.X)\n",
        "    print('data.y=', data.y)"
      ],
      "metadata": {
        "id": "h6bBgskcrcYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "data.X が3次元データになっているのがわかるでしょう．\n",
        "また数値が scaler によって基準化されている点にも注意してください．"
      ],
      "metadata": {
        "id": "aY_YiKYisIjO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "クラス SPData で作られた\n",
        "data.X, data.y\n",
        "を材料にして，訓練用データとテスト用データを構築するのが，\n",
        "つぎのクラス SPDataConstructor です．"
      ],
      "metadata": {
        "id": "TVhEcaLgsKLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SPDataConstructor(object):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def construct(self, test_size=0.2):\n",
        "        \"\"\"\n",
        "        Params:\n",
        "            test_size (float): ratio for test data, default is 0.2 (20% testing data)\n",
        "        \"\"\"\n",
        "        self.test_size = test_size\n",
        "\n",
        "        # split the dataset into training & testing sets by date (not randomly splitting)\n",
        "        train_samples = int((1 - self.test_size) * len(data.X)) # number of in-samples\n",
        "        X_train = data.X[:train_samples] # shape(X_train)= (train_samples) x ((n_steps) x (# of feature_colums +1)\n",
        "        self.y_train = data.y[:train_samples]\n",
        "        X_test = data.X[train_samples:]\n",
        "        self.y_test = data.y[train_samples:]\n",
        "\n",
        "        self.dates = X_test[:, -1, -1] # get the list of test set dates (the last days of each sequence)\n",
        "        # retrieve test features from the original dataframe\n",
        "        self.test_df = self.data.orig_df.loc[self.dates]\n",
        "        # remove duplicated dates in the testing dataframe\n",
        "        self.test_df = self.test_df[~self.test_df.index.duplicated(keep='first')]\n",
        "        # remove 'date' fields from the training/testing sets & convert to float32\n",
        "        self.X_train = X_train[:, :, :len(self.data.feature_columns)].astype(np.float32)\n",
        "        self.X_test = X_test[:, :, :len(self.data.feature_columns)].astype(np.float32)\n",
        "\n",
        "        return((self.X_train, self.X_test, self.y_train, self.y_test))"
      ],
      "metadata": {
        "id": "Xj7vfoJtsO-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ここで行うデータ構成方法としては，\n",
        "\n",
        "1. データ全体を前半と後半に分け，それぞれを訓練用とテスト用に使う\n",
        "1. 最初の1年分を訓練用としそれに続く1ヶ月をテスト用とする．さらに，一月後ろにずらしてつぎの1年分の訓練用データとそれに続く1ヶ月のテスト用データを構築．以下，これを繰り返す．\n",
        "\n",
        "のように，様々な手法が考えられます．\n",
        "ここでは，データ全体の前\n",
        "1- test_size $\\in[0, 1]$ を訓練データに，\n",
        "残りをテスト・データとしています．"
      ],
      "metadata": {
        "id": "I5ShqakxsY7k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**課題**\n",
        "X_train, y_train, X_test, y_test を Numpy array で実装されている．これは，ビッグデータやリアルタイム・データを扱う場合には利用できない．(何故か？) そこでこれら4つのオブジェクトを，[generator](https://wiki.python.org/moin/Generators) として実装してみよ．"
      ],
      "metadata": {
        "id": "TDibtcFmsid0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2 ナイーブな予測**\n",
        "\n",
        "この節では，機械学習を使わずに，lookup_step日後の株価を予測するナイーブな方法を考えます．\n",
        "\n",
        "まずは，\n",
        "前節で構築した data_c.X_test と data_c.y_test\n",
        "の内容を見てみましょう．"
      ],
      "metadata": {
        "id": "euSjDwhPsyZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    data_c = SPDataConstructor(data)\n",
        "    data_c.construct(test_size=0.2)\n",
        "\n",
        "    print('X_test=')\n",
        "    print(data_c.X_test)\n",
        "    print('X_test[-2]=', data_c.X_test[-2])\n",
        "    print('X_test[-1]=', data_c.X_test[-1])\n",
        "    pred_p = data_c.X_test[:, -1, 0]\n",
        "    print('pred_p=', pred_p)\n",
        "    print('y_test=', data_c.y_test)"
      ],
      "metadata": {
        "id": "Tt0n95hXsZ93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ここで pred_p は，X_test の各日付の最終日の終値を表しています．\n",
        "ナイーブな予測として，この pred_p の値を lookup_step日後の株価予測値とします．\n",
        "そしてその予測値と y_test の値との差の絶対値の平均値，\n",
        "すなわち Mean Absolute Error (MAE) を計算し，予測精度を評価します．\n",
        "\n",
        "このナイーブ予測評価を行うクラス SPNaiveModel を以下に示します．"
      ],
      "metadata": {
        "id": "rRi9sEFatDLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class SPNaiveModel(object):\n",
        "    def __init__(self, data_c):\n",
        "        \"\"\"\n",
        "        Params:\n",
        "            data_c (SPDataConstructor): the data you are anlyzing\n",
        "        \"\"\"\n",
        "        self.data_c = data_c\n",
        "        self.data = data_c.data\n",
        "\n",
        "        self.pred_p = self.data_c.X_test[:, -1, 0]\n",
        "        self.true_p = self.data_c.y_test\n",
        "        self.mae, self.mae_scaled = self.get_mae(self.true_p, self.pred_p)\n",
        "\n",
        "    def examine(self):\n",
        "        self.add_true_pred_p(self.true_p, self.pred_p)\n",
        "        self.plot_true_pred_comparison()\n",
        "        print('naive_MAE=', self.mae)\n",
        "        print(\"naive_MAE_scaled=\", self.mae_scaled)\n",
        "\n",
        "    def get_mae(self, targets, preds): # compute MAE\n",
        "        \"\"\"\n",
        "        Params:\n",
        "            targets: true values\n",
        "            preds: predicted values\n",
        "        \"\"\"\n",
        "        batch_maes = []\n",
        "        for setp in range(len(targets)):\n",
        "            mae = np.mean(np.abs(preds - targets))\n",
        "            batch_maes.append(mae)\n",
        "        mae_scaled = np.mean(batch_maes)\n",
        "        mae = self.data.scaler.inverse_transform([[mae_scaled]])[0][0]\n",
        "        return([mae, mae_scaled])\n",
        "\n",
        "    def add_true_pred_p(self, y_test, y_pred):\n",
        "        \"\"\"\n",
        "        Params:\n",
        "            y_test: array of true values\n",
        "            y_pred: array of predicted values\n",
        "        \"\"\"\n",
        "        test_df = self.data_c.test_df\n",
        "        # add true future prices to the dataframe\n",
        "        test_df[\"true_adjclose_%d\" % data.lookup_step] = y_test\n",
        "        # add naively-predicted future prices to the dataframe\n",
        "        test_df[\"adjclose_naive\"] = self.data.scaler.inverse_transform([y_pred])[0]\n",
        "\n",
        "        # sort the dataframe by date\n",
        "        test_df.sort_index(inplace=True)\n",
        "\n",
        "        self.true_p = test_df[\"true_adjclose_%d\" % data.lookup_step]\n",
        "        self.naive_pred_p =  test_df[\"adjclose_naive\"]\n",
        "        return(test_df)\n",
        "\n",
        "    # plot true close price along with predicted close price\n",
        "    def plot_true_pred_comparison(self):\n",
        "        f = plt.figure()\n",
        "        f.set_size_inches(10, 7)\n",
        "        plt.plot(self.true_p, label=\"Actual Price\")\n",
        "        dates = self.true_p.index\n",
        "        plt.plot(dates, self.pred_p, label=\"Naively predicted Price\")\n",
        "        plt.title(\"Naive prediction(%s)\" % self.data.ticker)\n",
        "        plt.xlabel(\"days\")\n",
        "        plt.ylabel(\"price\")\n",
        "        plt.legend(loc='upper left', borderaxespad=3.0, fontsize=9)\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "SYG5isFStEJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "X_test の最終日の終値を pred_p に，\n",
        "また y_test の値を真値 true_p として，\n",
        "MAEを計算し，グラフに表示しています．\n",
        "\n",
        "実行結果は以下のとおりです．"
      ],
      "metadata": {
        "id": "z4tPJM9ItNs4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    ### construct a non-ML model ###\n",
        "    naive_model = SPNaiveModel(data_c)\n",
        "    naive_model.examine()"
      ],
      "metadata": {
        "id": "3Sh0U2uMtO6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "上のグラフは scaled value で表示されていますが，n_steps 日ずれて予測値がプロットされているのがわかるでしょう．\n",
        "\n",
        "一方，scaled MAEの 0.048 という値は，かなりよくて，これを打ち負かすのは，至難の業ということがいずれわかります．"
      ],
      "metadata": {
        "id": "1hRRltD9tcoF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**課題**\n",
        "上記のグラフを unscaled の値で表示してみよ．"
      ],
      "metadata": {
        "id": "agfc0tRk1Tyw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3 深層学習による予測**\n",
        "\n",
        "この節以降では，Keras の深層学習ライブラリを利用して，前節のナイーブ予測を打ち負かす株価予測に挑戦します．"
      ],
      "metadata": {
        "id": "KRoEZe_N1aHF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "前節までにトレーニングで利用した X_train や y_train は日付順に並んだデータでした．\n",
        "しかし，ともするとこの日付順という暗黙の情報は学習に不利に働くことがあります．\n",
        "このため，データを日付でシャッフルすることで，予測精度を向上させることができるかもしれません．\n",
        "\n",
        "そこで，先に定義したクラス SPDataConstructor のコンストラクタの引数にブール値をとる shuffle を追加します．"
      ],
      "metadata": {
        "id": "KYA6nKye1iM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "class SPDataConstructor(object):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def construct(self, shuffle=True, test_size=0.2):\n",
        "        \"\"\"\n",
        "        Params:\n",
        "            shuffle (bool): whether to shuffle the dataset (both training & testing), default is True\n",
        "            test_size (float): ratio for test data, default is 0.2 (20% testing data)\n",
        "        \"\"\"\n",
        "        self.shuffle = shuffle\n",
        "        self.test_size = test_size\n",
        "\n",
        "        # split the dataset into training & testing sets by date (not randomly splitting)\n",
        "        train_samples = int((1 - self.test_size) * len(data.X)) # number of in-samples\n",
        "        X_train = data.X[:train_samples] # shape(X_train)= (train_samples) x ((n_steps) x (# of feature_colums +1)\n",
        "        self.y_train = data.y[:train_samples]\n",
        "        X_test = data.X[train_samples:]\n",
        "        self.y_test = data.y[train_samples:]\n",
        "        if self.shuffle:\n",
        "            self.shuffle_database(X_train, self.y_train, X_test, self.y_test)\n",
        "\n",
        "        self.dates = X_test[:, -1, -1] # get the list of test set dates (the last days of each sequence)\n",
        "        # retrieve test features from the original dataframe\n",
        "        self.test_df = self.data.orig_df.loc[self.dates]\n",
        "        # remove duplicated dates in the testing dataframe\n",
        "        self.test_df = self.test_df[~self.test_df.index.duplicated(keep='first')]\n",
        "        # remove 'date' fields from the training/testing sets & convert to float32\n",
        "        self.X_train = X_train[:, :, :len(self.data.feature_columns)].astype(np.float32)\n",
        "        self.X_test = X_test[:, :, :len(self.data.feature_columns)].astype(np.float32)\n",
        "\n",
        "        return((self.X_train, self.X_test, self.y_train, self.y_test))\n",
        "\n",
        "    def shuffle_database(self, X_train, y_train, X_test, y_test):\n",
        "        state = np.random.get_state()\n",
        "        np.random.shuffle(X_train)\n",
        "        np.random.set_state(state)\n",
        "        np.random.shuffle(y_train)\n",
        "        state = np.random.get_state()\n",
        "        np.random.shuffle(X_test)\n",
        "        np.random.set_state(state)\n",
        "        np.random.shuffle(y_test)"
      ],
      "metadata": {
        "id": "TkS82nzrtdfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ではいよいよ，TensorFlow2 と Keras を使うことにします．\n",
        "まずは，関係するライブラリを import します．"
      ],
      "metadata": {
        "id": "5ecEXV5t1zaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import activations\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import metrics\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard"
      ],
      "metadata": {
        "id": "eOn3YXUr10Z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "次にモデルを実装するクラス SPModel を導入します．"
      ],
      "metadata": {
        "id": "Ip2Sg8Bq19jo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# stock price model constructed with a recurrent neural network\n",
        "class SPModel(object):\n",
        "    def __init__(self, data_c, loss=losses.MeanAbsoluteError(), optimizer=optimizers.RMSprop()):\n",
        "        \"\"\"\n",
        "        Params:\n",
        "            data_c (SPDataConstructor): the data you are anlyzing.\n",
        "            loss (loss function): see https://keras.io/api/losses/\n",
        "            optimizer (optimizer object): see https://keras.io/api/optimizers/\n",
        "        \"\"\"\n",
        "\n",
        "        self.data_c = data_c\n",
        "        self.data = data_c.data\n",
        "        self.loss = loss\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "    # build and compile model.\n",
        "    def build_model(self, metrics=metrics.Accuracy()):\n",
        "        \"\"\"\n",
        "        Params:\n",
        "            matrics (matrics object): see https://keras.io/api/metrics/\n",
        "        \"\"\"\n",
        "        # model name to save, making it as unique as possible based on parameters\n",
        "        date_now = time.strftime(\"%Y-%m-%d\")\n",
        "        shuffle_str = \"sh-%d\" % data_c.shuffle\n",
        "        self.name = \"%s_%s-%s\" % (date_now, data.ticker, shuffle_str)\n",
        "        self.model = models.Sequential()\n",
        "        self.build()\n",
        "        self.model.compile(loss=self.loss, metrics=metrics, optimizer=self.optimizer)\n",
        "\n",
        "    def build(self): # virtual method\n",
        "        pass\n",
        "\n",
        "    # fit model.\n",
        "    def fit(self, batch_size, epochs):\n",
        "        \"\"\"\n",
        "        Params:\n",
        "            batch (int): the size of batch.\n",
        "            epochs (int): the number of epochs.\n",
        "        \"\"\"\n",
        "        # some tensorflow callbacks\n",
        "        # see https://keras.io/api/callbacks/\n",
        "        # train the model and save the weights whenever we see\n",
        "        # a new optimal model using ModelCheckpoint\n",
        "        checkpointer = ModelCheckpoint(os.path.join(\"results\", self.name + \".h5\"), save_weights_only=True, save_best_only=True, verbose=1)\n",
        "        tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", self.name))\n",
        "        #tensorboard --logdir=\"logs\"\n",
        "        self.history = self.model.fit(self.data_c.X_train, self.data_c.y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    validation_data=(self.data_c.X_test, self.data_c.y_test),\n",
        "                    callbacks=[checkpointer, tensorboard],\n",
        "                    verbose=1)\n",
        "        self.loss = self.history.history['loss']\n",
        "        self.val_loss = self.history.history['val_loss']\n",
        "        return(self.history)"
      ],
      "metadata": {
        "id": "BWvHH-XQ1-Vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SPModel のコンストラクタの引数は，\n",
        "\n",
        "1. 先に作った DataConstructor のインスタンス\n",
        "1. loss関数\n",
        "1. optimizer\n",
        "\n",
        "の3つで，その中身は，これらをデータメンバとして登録するだけです．\n",
        "\n",
        "モデルを構築しコンパイルするメソッドは\n",
        "build_model\n",
        "です．このメソッドは metric関数を引数として持ちます．\n",
        "metric関数は loss関数と似ていますが，訓練の際ではなくコンパイル時に使われるところが異なります．\n",
        "\n",
        "実際のモデル構成は\n",
        "build_model\n",
        "のなかから呼ぶメソッド build で行います．\n",
        "しかし SPModel ではこれは仮想メソッドとなっていて，実装は派生クラスで行うことになります．\n",
        "\n",
        "モデル構成が終わったあと，コンパイルしています．\n",
        "\n",
        "訓練は fit メソッドで行います．\n",
        "このメソッドは\n",
        "batchサイズと\n",
        "繰り返し数 epochs\n",
        "を引数として持ちます．\n",
        "\n",
        "中身は Keras の model.fit を呼ぶところがだけが本質的ですが，\n",
        "中間結果を\n",
        "resultsフォルダと\n",
        "logsフォルダに\n",
        "毎epoch格納するためのコールバックを登録しています．\n",
        "また，fit が終わったあと，\n",
        "in-sample のエラー (val_loss)\n",
        "と\n",
        "out-of-sample のエラー (loss)\n",
        "をデータメンバとして保持します．"
      ],
      "metadata": {
        "id": "xOq7sSd72FjJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1 Flatten**\n",
        "\n",
        "それでは SPModel の build メソッドを具体化した最初の例として，\n",
        "SPModel から派生させたクラス\n",
        "SPFlatten\n",
        "を作成しましょう．\n",
        "深層学習の最初の層を\n",
        "Flatten層\n",
        "にするこの例は，数ある深層学習モデルのベースラインとなるものです．"
      ],
      "metadata": {
        "id": "8doBAAYI2LuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SPFlatten(SPModel):\n",
        "    def __init__(self, data_c, loss=losses.MeanAbsoluteError(), optimizer=optimizers.RMSprop(), neurons=32, activation=activations.sigmoid):\n",
        "        \"\"\"\n",
        "        Params:\n",
        "            data_c (SPDataConstructor): the data you are anlyzing\n",
        "            loss (loss function): see https://keras.io/api/losses/\n",
        "            optimizer (optimizer object): see https://keras.io/api/optimizers/\n",
        "            neurons (int): the number of artificial neurons per each layer, default is 32\n",
        "            activation (activation object): see https://keras.io/ja/activations/\n",
        "        \"\"\"\n",
        "        super().__init__(data_c, loss, optimizer)\n",
        "\n",
        "        self.neurons = neurons\n",
        "        self.activation = activation\n",
        "\n",
        "    def build(self):\n",
        "        layer = layers.Flatten\n",
        "        self.name += self.name + f\"-{self.loss.name}-{self.activation.__name__}-{self.optimizer.name}-{layer.__name__}-seq-{data.n_steps}-step-{data.lookup_step}-neurons-{self.neurons}\"\n",
        "\n",
        "        n_features = len(data.feature_columns)\n",
        "        self.model.add(layer(batch_input_shape=(None, data.n_steps, n_features)))\n",
        "        self.model.add(layers.Dense(self.neurons))\n",
        "\n",
        "        self.model.add(layers.Dense(1, activation=self.activation))"
      ],
      "metadata": {
        "id": "XXMfS51g2GXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SPFlatten\n",
        "のコンストラクタの引数は，\n",
        "SPModel\n",
        "のそれに加えて\n",
        "ニューロン数 neurons\n",
        "と\n",
        "活性化関数 activation\n",
        "を持ちます．\n",
        "\n",
        "build メソッドで行う\n",
        "モデル構成は，入力層と出力層の間に中間層が1つある3層からなっています．\n",
        "\n",
        "入力層で使われている Flatten層は，つぎつぎと入力される batch データを独立なデータと捉えて学習させます．\n",
        "中間層と出力層は\n",
        "指定されたニューロン数と活性関数をそれぞれ使った\n",
        "全結合層(dense層)\n",
        "を用いています．"
      ],
      "metadata": {
        "id": "dpTA3ht12X05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "結果を格納するファイルのファイル名\n",
        "self.name\n",
        "の指定で用いられている\n",
        "loss.name, activation.\\__name__, optimaizer._name\n",
        "は，それぞれの名称を取り出すときに使うデータメンバが異なる点に注意してください．\n",
        "こうした細かい情報は API のドキュメントにはでていないことがあり，\n",
        "そういう場合は，GitHub にあるKerasの[ソースコード](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/keras)で確認する必要があります．"
      ],
      "metadata": {
        "id": "99cBNRFF2ckg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**課題**\n",
        "[GitHub](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/keras) を探索して，上で示した名称のデータメンバのいくつかが，どこで決まっているかを確認せよ．"
      ],
      "metadata": {
        "id": "GPdhvY092j1u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SPModel\n",
        "(の派生クラス)\n",
        "で作ったモデルの予測精度を評価するためのクラス\n",
        "SPPredict を用意します．"
      ],
      "metadata": {
        "id": "HpTjPHLF2pb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predict and evaluate the constructed model\n",
        "class SPPredict(object):\n",
        "    def __init__(self, naive_model, model):\n",
        "        \"\"\"\n",
        "        Params:\n",
        "            naive_model (SPNaiveModel): the naive data you are refering\n",
        "            model (SPModel): the data you are anlyzing\n",
        "        \"\"\"\n",
        "        self.naive_model = naive_model\n",
        "        self.model = model\n",
        "        self.data_c = self.model.data_c\n",
        "        self.data = self.model.data\n",
        "\n",
        "    def examine(self):\n",
        "        self.load_weights()\n",
        "        self.get_final_df() # get the final dataframe for the testing set\n",
        "        self.evaluate() # evaluate the model\n",
        "        self.print_metrics() # printing metrics\n",
        "        self.plot_true_pred_comparison() # plot true/predicted prices graph\n",
        "        self.plot_epochs_mae() # plot MAE graph\n",
        "        self.save_final_df() # save the final dataframe to csv-results folder\n",
        "\n",
        "    def load_weights(self):\n",
        "        # load optimal model weights from results folder\n",
        "        model_path = os.path.join(\"results\", self.model.name) + \".h5\"\n",
        "        self.model.model.load_weights(model_path)\n",
        "\n",
        "    def get_final_df(self):\n",
        "        \"\"\"\n",
        "        This function takes the `model` and `data` dict to\n",
        "        construct a final dataframe that includes the features along\n",
        "        with true and predicted prices of the testing dataset\n",
        "        \"\"\"\n",
        "\n",
        "        X_test = self.data_c.X_test\n",
        "        y_test = self.data_c.y_test\n",
        "        # perform prediction and get prices\n",
        "        y_pred = self.model.model.predict(X_test)\n",
        "        y_test_exp = np.expand_dims(y_test, axis=0)\n",
        "        self.mae, self.mae_scaled = self.naive_model.get_mae(y_test_exp, y_pred)\n",
        "        y_test = np.squeeze(self.data.scaler.inverse_transform(y_test_exp))\n",
        "        y_pred = np.squeeze(self.data.scaler.inverse_transform(y_pred))\n",
        "\n",
        "        self.final_df = self.add_true_pred_p(y_test, y_pred)\n",
        "\n",
        "    def evaluate(self): # evaluate the model\n",
        "        loss, mae = self.model.model.evaluate(self.data_c.X_test, self.data_c.y_test, verbose=0)\n",
        "        self.loss = loss\n",
        "        # calculate the mean absolute error (inverse scaling)\n",
        "        self.mean_absolute_error = mae\n",
        "\n",
        "    def add_true_pred_p(self, y_test, y_pred):\n",
        "        test_df = self.data_c.test_df\n",
        "        # add true future prices to the dataframe\n",
        "        test_df[\"true_adjclose_%d\" % data.lookup_step] = y_test\n",
        "        # add predicted future prices to the dataframe\n",
        "        test_df[\"adjclose_%d\" % data.lookup_step] = y_pred\n",
        "        # add naively-predicted future prices to the dataframe\n",
        "        test_df[\"adjclose_naive\"] = self.data.scaler.inverse_transform([self.data_c.X_test[:,-1,0]])[0]\n",
        "\n",
        "        # sort the dataframe by date\n",
        "        test_df.sort_index(inplace=True)\n",
        "\n",
        "        self.true_p = test_df[\"true_adjclose_%d\" % data.lookup_step]\n",
        "        self.pred_p = test_df[\"adjclose_%d\" % data.lookup_step]\n",
        "        self.naive_pred_p =  test_df[\"adjclose_naive\"]\n",
        "        return(test_df)\n",
        "\n",
        "    # plot true close price along with predicted close price\n",
        "    def plot_true_pred_comparison(self):\n",
        "        f = plt.figure()\n",
        "        f.set_size_inches(10, 7)\n",
        "        plt.plot(self.true_p, label=\"Actual Price\")\n",
        "        plt.plot(self.pred_p, label=\"Predicted Price\")\n",
        "        plt.plot(self.naive_pred_p, label=\"Naively predicted Price\")\n",
        "        plt.title(\"Prediction(%s)\" % self.data.ticker)\n",
        "        plt.xlabel(\"days\")\n",
        "        plt.ylabel(\"price\")\n",
        "        plt.legend(loc='upper left', borderaxespad=3.0, fontsize=9)\n",
        "        plt.show()\n",
        "\n",
        "    # plot true close price along with predicted close price\n",
        "    def plot_epochs_mae(self):\n",
        "        f = plt.figure()\n",
        "        f.set_size_inches(10, 7)\n",
        "        max_epochs = len(self.model.loss)\n",
        "        epochs = range(1, max_epochs + 1)\n",
        "        plt.plot(epochs, self.model.loss, 'bo', label=\"Training loss\")\n",
        "        plt.plot(epochs, self.model.val_loss, 'b', label=\"Validation loss\")\n",
        "        plt.hlines([self.naive_model.mae_scaled], 0, max_epochs, \"green\", linestyles='dashed', label='non-ML baseline')\n",
        "        plt.title('Training and validation loss')\n",
        "        plt.xlabel(\"epochs\")\n",
        "        plt.ylabel(\"mae\")\n",
        "        plt.legend(loc='upper right', borderaxespad=3.0, fontsize=9)\n",
        "        plt.show()\n",
        "\n",
        "    # print metrics\n",
        "    def print_metrics(self):\n",
        "        print(\"model.loss=\", self.model.loss)\n",
        "        print(\"loss:\", self.loss)\n",
        "        print(\"MAE:\", self.mae)\n",
        "        print(\"MAE_scaled:\", self.mae_scaled)\n",
        "        print('naive_MAE=', self.naive_model.mae)\n",
        "        print(\"naive_MAE_scaled=\", self.naive_model.mae_scaled)\n",
        "\n",
        "    # save the final dataframe to csv-results folder\n",
        "    def save_final_df(self):\n",
        "        csv_filename = os.path.join(\"csv-results\", self.model.name + \".csv\")\n",
        "        self.final_df.to_csv(csv_filename)"
      ],
      "metadata": {
        "id": "UXSC9lwe2Y_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SPPredict のコンストラクタは，\n",
        "naiveモデル (非機械学習モデル)\n",
        "のインスタンス\n",
        "naive_model\n",
        "と\n",
        "SPModel (機械学習モデル)\n",
        "のインスタンス\n",
        "model\n",
        "を引数として持ちます．\n",
        "\n",
        "メソッド\n",
        "examine\n",
        "のなかで，個々の評価用サブメソッドを実行します．\n",
        "\n",
        "最初は，results フォルダから訓練済みのパラメタを読み込む\n",
        "load_weights メソッド．\n",
        "そこからテストデータとその予測値を含んだデータベースを復元し\n",
        "(get_final_df)，\n",
        "各種指標と図表を表示させています．"
      ],
      "metadata": {
        "id": "MAxnVYUk2zW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "以上のコードを走らせるために，Naive model でも使った乱数シードの設定関数とフォルダの設定関数を以下のようにアップグレードします．"
      ],
      "metadata": {
        "id": "pAqmG-0Q27QJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Miscellaneous Functions\n",
        "\n",
        "def set_seeds(): # set seeds, so we can get the same results after rerunning several times\n",
        "    np.random.seed(314)\n",
        "    random.seed(314)\n",
        "    tf.random.set_seed(314)\n",
        "\n",
        "\n",
        "def create_folders(): # create folders if they do not exist\n",
        "    if not os.path.isdir(\"data\"):\n",
        "        os.mkdir(\"data\")\n",
        "    if not os.path.isdir(\"results\"):\n",
        "        os.mkdir(\"results\")\n",
        "    if not os.path.isdir(\"logs\"):\n",
        "        os.mkdir(\"logs\")\n",
        "    if not os.path.isdir(\"csv-results\"):\n",
        "        os.mkdir(\"csv-results\")"
      ],
      "metadata": {
        "id": "C2hOyuY828at"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "実行するためのメインプログラムは以下のようになります．"
      ],
      "metadata": {
        "id": "N6jrmmZC3DJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    set_seeds()\n",
        "    create_folders()\n",
        "\n",
        "    ### load the data ###\n",
        "\n",
        "    ticker = \"GOOGL\"\n",
        "    #ticker = \"AAPL\"\n",
        "    #ticker = \"META\"  # ex-FB\n",
        "    #ticker = \"AMZN\"\n",
        "    data = SPData(ticker)\n",
        "\n",
        "    #data.set_params(shuffle=True, test_size=0.2)\n",
        "\n",
        "    # for scaler, see https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing\n",
        "    SCALER = preprocessing.MinMaxScaler()   # fit the data within [0,1]\n",
        "    #SCALER = preprocessing.StandardScaler()    # normalize the data\n",
        "    data.preprocess_data(scaler=SCALER, lookup_step=15, n_steps=50)\n",
        "    data_c = SPDataConstructor(data)\n",
        "    data_c.construct(shuffle=True, test_size=0.2)\n",
        "\n",
        "    ### construct a non-ML model ###\n",
        "    naive_model = SPNaiveModel(data_c)\n",
        "\n",
        "    ### construct the model ###\n",
        "\n",
        "    # for loss function, see https://keras.io/api/losses/\n",
        "    LOSS = losses.MeanAbsoluteError()\n",
        "    #LOSS = losses.Huber()\n",
        "    #LOSS = losses.BinaryCrossentropy()\n",
        "\n",
        "    # for optimizer, see https://keras.io/api/optimizers/\n",
        "    #OPTIMIZER = optimizers.RMSprop()\n",
        "    OPTIMIZER = optimizers.Adam()\n",
        "\n",
        "    ## for activation function, see https://keras.io/ja/activations/\n",
        "    # for activation function, see https://keras.io/api/layers/activations/\n",
        "    #ACTIVATION = activations.relu\n",
        "    #ACTIVATION = activations.sigmoid\n",
        "    ACTIVATION = activations.linear\n",
        "\n",
        "    model = SPFlatten(data_c, loss=LOSS, optimizer=OPTIMIZER, neurons=32, activation=ACTIVATION)\n",
        "\n",
        "    # for metrics, see https://keras.io/api/metrics/\n",
        "    #METRICS = metrics.Accuracy()\n",
        "    METRICS = metrics.MeanAbsoluteError()\n",
        "    model.build_model(METRICS)\n",
        "\n",
        "    history = model.fit(batch_size=64, epochs=10)\n",
        "\n",
        "    orig_loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    epochs = range(1, len(val_loss) + 1)\n",
        "    print('epochs=', epochs)\n",
        "    print('loss=', orig_loss)\n",
        "    print('val_loss=', val_loss)\n",
        "\n",
        "    ### examine the trained model ###\n",
        "\n",
        "    pred = SPPredict(naive_model, model)\n",
        "    pred.examine()"
      ],
      "metadata": {
        "id": "utEgfeYx3D_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "一番下の図の中の緑の破線は，naiveモデルでの mean abosolute error を表しています．これよりも下にあれば，naive モデルより予測精度の良いモデルということになります．\n",
        "\n",
        "in-sample の trading loss こそ下回っていますが，\n",
        "out-of-sample の validation loss は破線の上側にあり，\n",
        "naiveモデルに負けていることがわかります．"
      ],
      "metadata": {
        "id": "XYwHXIht3X18"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.2 GRU**\n",
        "\n",
        "Flatten層では時間情報を潰してしまっていましたが，\n",
        "これを直前の batch data の処理後のデータを参照することによって\n",
        "過去のデータに依存した処理を行えるようにした\n",
        "recurrent neural network (RNN)\n",
        "に置き換えることによって，予測精度を向上させることを考えてみます．\n",
        "\n",
        "ここでは，RNN のひとつである GRU層を使ったクラス\n",
        "SPGRU を実装してみます．"
      ],
      "metadata": {
        "id": "Zk4HqyrV3dp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SPGRU(SPModel):\n",
        "    def __init__(self, data_c, loss=losses.MeanAbsoluteError(), optimizer=optimizers.RMSprop(), neurons=32, activation=activations.sigmoid):\n",
        "        \"\"\"\n",
        "        Params:\n",
        "            data_c (SPDataConstructor): the data you are anlyzing\n",
        "            loss (loss function): see https://keras.io/api/losses/\n",
        "            optimizer (optimizer object): see https://keras.io/api/optimizers/\n",
        "            neurons (int): the number of artificial neurons per each layer, default is 32\n",
        "            activation (activation object): see https://keras.io/ja/activations/\n",
        "        \"\"\"\n",
        "        super().__init__(data_c, loss, optimizer)\n",
        "\n",
        "        self.neurons = neurons\n",
        "        self.activation = activation\n",
        "\n",
        "    def build(self):\n",
        "        layer = layers.GRU\n",
        "        self.name += self.name + f\"-{self.loss.name}-{self.activation.__name__}-{self.optimizer.name}-{layer.__name__}-seq-{data.n_steps}-step-{data.lookup_step}-neurons-{self.neurons}\"\n",
        "\n",
        "        n_features = len(data.feature_columns)\n",
        "        self.model.add(layer(self.neurons,\n",
        "                            dropout=0.2,\n",
        "                            recurrent_dropout=0.2,\n",
        "                            return_sequences=False,\n",
        "                            batch_input_shape=(None, data.n_steps, n_features)))\n",
        "        #self.model.add(layer(self.neurons, return_sequences=False))\n",
        "        self.model.add(layers.Dense(1, activation=self.activation))"
      ],
      "metadata": {
        "id": "6_WXqMT-3Ysv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SPGRU のコンストラクタは，SPFlatten のそれとまったく同じで，\n",
        "build メソッドの中で構築されるモデルのみが異なります．\n",
        "\n",
        "今回は2層にしていて，入力層にGRU層を用いています．\n",
        "ここで\n",
        "dropout\n",
        "と\n",
        "recurrent_dropout\n",
        "は過学習を低減するためのパラメタです．"
      ],
      "metadata": {
        "id": "y-TSFak63oOp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GRUモデルを走らせるためのメインプログラムは，Flattenモデルのそれを1行だけ変更したものです．"
      ],
      "metadata": {
        "id": "ocsiQm_y3tZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    set_seeds()\n",
        "    create_folders()\n",
        "\n",
        "    ### load the data ###\n",
        "\n",
        "    #ticker = \"GOOGL\"\n",
        "    #ticker = \"AAPL\"\n",
        "    ticker = \"META\" # ex-FB\n",
        "    #ticker = \"AMZN\"\n",
        "    data = SPData(ticker)\n",
        "\n",
        "    #data.set_params(shuffle=True, test_size=0.2)\n",
        "\n",
        "    # for scaler, see https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing\n",
        "    SCALER = preprocessing.MinMaxScaler()   # fit the data within [0,1]\n",
        "    #SCALER = preprocessing.StandardScaler()    # normalize the data\n",
        "    data.preprocess_data(scaler=SCALER, lookup_step=15, n_steps=50)\n",
        "    data_c = SPDataConstructor(data)\n",
        "    data_c.construct(shuffle=True, test_size=0.2)\n",
        "\n",
        "    ### construct a non-ML model ###\n",
        "    naive_model = SPNaiveModel(data_c)\n",
        "\n",
        "    ### construct the model ###\n",
        "\n",
        "    # for loss function, see https://keras.io/api/losses/\n",
        "    LOSS = losses.MeanAbsoluteError()\n",
        "    #LOSS = losses.Huber()\n",
        "    #LOSS = losses.BinaryCrossentropy()\n",
        "\n",
        "    # for optimizer, see https://keras.io/api/optimizers/\n",
        "    #OPTIMIZER = optimizers.RMSprop()\n",
        "    OPTIMIZER = optimizers.Adam()\n",
        "\n",
        "    ## for activation function, see https://keras.io/ja/activations/\n",
        "    # for activation function, see https://keras.io/api/layers/activations/\n",
        "    #ACTIVATION = activations.relu\n",
        "    #ACTIVATION = activations.sigmoid\n",
        "    ACTIVATION = activations.linear\n",
        "\n",
        "    model = SPGRU(data_c, loss=LOSS, optimizer=OPTIMIZER, neurons=32, activation=ACTIVATION)\n",
        "\n",
        "    # for metrics, see https://keras.io/api/metrics/\n",
        "    #METRICS = metrics.Accuracy()\n",
        "    METRICS = metrics.MeanAbsoluteError()\n",
        "    model.build_model(METRICS)\n",
        "\n",
        "    history = model.fit(batch_size=64, epochs=10)\n",
        "\n",
        "    orig_loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    epochs = range(1, len(val_loss) + 1)\n",
        "    print('epochs=', epochs)\n",
        "    print('loss=', orig_loss)\n",
        "    print('val_loss=', val_loss)\n",
        "\n",
        "    ### examine the trained model ###\n",
        "\n",
        "    pred = SPPredict(naive_model, model)\n",
        "    pred.examine()"
      ],
      "metadata": {
        "id": "5dJ0BJKK3pNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "残念ながら，このモデルでも，naiveモデルを打ち負かすことはできていません．"
      ],
      "metadata": {
        "id": "FUHR_7ir4EZh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**課題**\n",
        "工夫して，より高精度な予測を行うモデルを探せ．\n",
        "\n",
        "例えば，以下のような点の変更が考えられるが，これらがすべてではない．\n",
        "\n",
        "1. [optimizer](https://keras.io/api/optimizers/) (RMSprop, Adam 等）\n",
        "1. [metrics](https://keras.io/api/metrics/) (Accuracy, MeanAbsoluteError 等)\n",
        "1. [layer](https://keras.io/ja/layers/recurrent/)(SimpleRNN, GRU, LSTM 等)\n",
        "1. [loss関数](https://keras.io/api/losses/)(MAE, Huber, Crossentropy 等)\n",
        "1. [activation関数](https://keras.io/api/layers/activations/)(relu, sigmoid, linear 等)\n",
        "1. dropout および recurrent_dropout\n",
        "1. neuron数\n",
        "1. layerの段数と構成\n",
        "1. epoch数\n",
        "1. shuffle のあるなし\n",
        "1. [scaler](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing) (MinMaxScaler, StandardScaler 等）\n",
        "\n",
        "また上に挙げた変更可能なパーツの多くは，独自に改良したコードに置き換えることも可能である．この場合は，[GitHub](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/keras) を参考にされたい．"
      ],
      "metadata": {
        "id": "56dKEwUW4JoT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**課題**\n",
        "上記課題を，複数の銘柄 (ticker）で行い，見つけたモデルの堅牢性，つまり，一般的にどの程度利用可能かを検討せよ．"
      ],
      "metadata": {
        "id": "bkHHBQ394Rdd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**課題**\n",
        "SPDataConsturctor は，全データから1組だけ訓練データとテストデータをとっているが，現実のように取引を継続して行う場合には，異なる抽出方法で評価する必要があるだろう．\n",
        "そうした評価方法を考え，実装せよ．"
      ],
      "metadata": {
        "id": "lGJ2vUuG4dKS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**課題**\n",
        "baseline (naive model) よりも大きく精度が高い予測を行おうとするには，今回の問題のセッティングのどこを再考すればよいか議論せよ．"
      ],
      "metadata": {
        "id": "Z0CtVtwh4VSR"
      }
    }
  ]
}